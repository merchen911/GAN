import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pds

import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow import keras
from tensorflow.examples.tutorials.mnist import input_data

config = tf.ConfigProto()
config.gpu_options.allow_growth = True 
sess = tf.Session(config=config)
tf.keras.backend.set_session(session=sess)


def relu(args):
    return tf.nn.leaky_relu(args,alpha=0.2)

def make_disciriminator(data_dim,discriminator_rate,num_class):
    input_layer=keras.Input(data_dim)
    dense_layer1=keras.layers.Dense(400,
                                   activation=relu)(input_layer)
    dense_layer2=keras.layers.Dense(100,
                                   activation=relu)(dense_layer1)
    output1=keras.layers.Dense(1,activation='sigmoid')(dense_layer2)
    model_mlp=keras.models.Model(input_layer,output1)
    
    label=keras.Input(1)
    label_embedding=keras.layers.Flatten()(keras.layers.Embedding(num_class,data_dim)(label))
    model_input=keras.layers.multiply([input_layer,label_embedding])
    
    validity=model_mlp(model_input)
    
    model=keras.models.Model([input_layer,label],validity)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=discriminator_rate,beta_1=0.6),
                 loss='binary_crossentropy',
                 metrics=['accuracy'])
    return model

def make_generator(latent_dim,data_dim,num_class):
    input_layer=keras.Input(latent_dim)
    dense_layer1=keras.layers.Dense(latent_dim*2,
                                   activation=relu)(input_layer)
    batch_layer1=keras.layers.BatchNormalization(momentum=0.8)(dense_layer1)
    dense_layer2=keras.layers.Dense(latent_dim*4,
                                   activation=relu)(batch_layer1)
    batch_layer2=keras.layers.BatchNormalization(momentum=0.8)(dense_layer2)
    output1=keras.layers.Dense(data_dim,activation='tanh')(batch_layer2)
    model_mlp=keras.models.Model(input_layer,output1)
    
    label = keras.Input(shape=(1,), dtype='int32')
    label_embedding = keras.layers.Flatten()(keras.layers.Embedding(num_class, latent_dim)(label))
    model_input = keras.layers.multiply([input_layer, label_embedding])

    gen_data=model_mlp(model_input)
    
    model=keras.models.Model([input_layer,label],gen_data)
    
    return model

def make_combine(latent_dim,generator,discriminator,generator_rate):
    discriminator.trainable=False
    input_value=keras.Input((latent_dim,))
    label=keras.Input(1)
    gen_value=generator([input_value,label])
    validity=discriminator([gen_value,label])
    
    model=keras.models.Model([input_value,label],validity)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=generator_rate,beta_1=0.6),
                 loss='binary_crossentropy')
    return model

def display(epoch,generator,latent_dim):
    fig,ax=plt.subplots(10,10,figsize=(11,10))
    fig.subplots_adjust(hspace=0.1)

    
    for row in range(10):
        for col in range(10):
            ax[row][col].imshow(generator.predict([np.random.normal(size=(1,latent_dim)),np.array([row])]).reshape(28,28))
            ax[row][col].set_xticks([])
            ax[row][col].set_yticks([])
            
    fig.suptitle('epoch : %d'%epoch)
    plt.show()
    plt.close()


# parameters
epochs = 130
n_batch=10
latent_dim=100
d_rate=0.0005
g_rate=0.001
drop_out_rate=0.2


data = input_data.read_data_sets("MNIST_data/", one_hot=True)
raw=data.train.images
target=data.train.labels.argmax(axis=1)

raw=(raw-0.5)/0.5
num_class=np.unique(target).shape[0]
batch_size=raw.shape[0]//n_batch
data_dim=raw.shape[1]

generator=make_generator(latent_dim,data_dim,num_class)
discriminator=make_disciriminator(data_dim,d_rate,num_class)
component=make_combine(latent_dim,generator,discriminator,g_rate)

valid=np.ones((batch_size,1))
fake=np.zeros((batch_size,1))
info=pds.DataFrame()
q=0


for _ in range(epochs):
    i=pds.DataFrame()
    
    for n in range(n_batch):
        idx=np.random.choice(target.shape[0],batch_size,replace=False)
        picked_data,picked_label=raw[idx,:],target[idx]
        
        for _ in range(3):
            noise=np.random.normal(0,1,(batch_size,latent_dim))
            gen_data=generator.predict([noise,picked_label])
            d_real_loss=discriminator.train_on_batch([picked_data,picked_label],valid)
            d_fake_loss=discriminator.train_on_batch([gen_data,picked_label],fake)
            d_loss=0.5*(d_real_loss[0]+d_fake_loss[1])
            
        noise=np.random.normal(0,1,(batch_size,latent_dim))
        gen_labels=np.random.randint(0,num_class,batch_size).reshape(-1,1)
        g_loss=component.train_on_batch([noise,gen_labels],valid)
        
        i.loc[n,'d_real_loss']=d_real_loss[0]
        i.loc[n,'d_real_acc']=d_real_loss[1]
        i.loc[n,'d_fake_loss']=d_fake_loss[0]
        i.loc[n,'d_fake_acc']=d_fake_loss[1]
        i.loc[n,'g_loss']=g_loss
        
    k=i.mean(axis=0)
    info.loc[q,'d_real_loss']=k.iloc[0]
    info.loc[q,'d_real_acc']=k.iloc[1]
    info.loc[q,'d_fake_loss']=k.iloc[2]
    info.loc[q,'d_fake_acc']=k.iloc[3]
    info.loc[q,'gloss']=k.iloc[4]
    del i,k
    display(q+1,generator,latent_dim)
    q+=1


fig,ax=plt.subplots(1,2,figsize=(10,4))
fig.subplots_adjust(hspace=0.3)
ax[0].plot(info.iloc[:,0].values.reshape(-1),'r',
              info.iloc[:,2].values.reshape(-1),'b',
              info.iloc[:,4].values.reshape(-1),'g',)
ax[0].legend(['d real','d fake','g'])
ax[0].set_xlabel('epoch')
ax[0].set_ylabel('loss')
ax[0].set_title('Loss')
ax[1].plot(info.iloc[:,1].values.reshape(-1),'r',
              info.iloc[:,3].values.reshape(-1),'b',)
ax[1].legend(['d real','d fake'])
ax[1].set_xlabel('epoch')
ax[1].set_ylabel('Accuracy')
ax[1].set_title('Acc')
fig.show()
